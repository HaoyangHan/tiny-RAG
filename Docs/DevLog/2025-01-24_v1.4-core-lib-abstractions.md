# TinyRAG v1.4 Core Library Abstractions Design

📅 **Date:** 2025-01-24  
🎯 **Version:** 1.4.0  
👨‍💻 **Developer:** AI Assistant  
📝 **Objective:** Design abstractions and concrete implementations for rag-memo-core-lib

---

## 🎯 **Design Philosophy**

### **Abstraction Principles**
- ✅ **Interface Segregation:** Small, focused interfaces for specific responsibilities
- ✅ **Dependency Inversion:** Depend on abstractions, not concretions
- ✅ **Open/Closed Principle:** Open for extension, closed for modification
- ✅ **Strategy Pattern:** Pluggable algorithms for different tenant types
- ✅ **Factory Pattern:** Creation of tenant-specific implementations

### **Technical Standards**
- **Python Version:** 3.10+
- **Type Annotations:** Strict typing with generics
- **Documentation:** Google-style docstrings
- **Testing:** Abstract test cases for all implementations
- **Async Support:** Full async/await support

---

## 🏗️ **Core Library Structure**

### **Enhanced Directory Layout**

```
rag-memo-core-lib/
├── src/
│   └── rag_memo_core_lib/
│       ├── __init__.py
│       ├── config/
│       │   ├── __init__.py
│       │   ├── settings.py                   # Core settings
│       │   ├── constants.py                  # Application constants
│       │   └── database.py                   # Database configuration
│       │
│       ├── abstractions/                     # Abstract base classes
│       │   ├── __init__.py
│       │   ├── base.py                       # Base abstractions
│       │   ├── llm.py                        # LLM provider abstractions
│       │   ├── vector_store.py               # Vector store abstractions
│       │   ├── document_processor.py         # Document processing abstractions
│       │   ├── generator.py                  # Generation abstractions
│       │   ├── evaluator.py                  # Evaluation abstractions
│       │   └── workflow.py                   # Workflow abstractions
│       │
│       ├── implementations/                  # Concrete implementations
│       │   ├── __init__.py
│       │   ├── llm/
│       │   │   ├── __init__.py
│       │   │   ├── openai_provider.py        # OpenAI implementation
│       │   │   ├── anthropic_provider.py     # Anthropic implementation
│       │   │   ├── ollama_provider.py        # Ollama implementation
│       │   │   └── mock_provider.py          # Mock for testing
│       │   ├── vector_stores/
│       │   │   ├── __init__.py
│       │   │   ├── qdrant_store.py           # Qdrant implementation
│       │   │   ├── chroma_store.py           # Chroma implementation
│       │   │   ├── faiss_store.py            # FAISS implementation
│       │   │   └── memory_store.py           # In-memory for testing
│       │   ├── processors/
│       │   │   ├── __init__.py
│       │   │   ├── pdf_processor.py          # PDF processing
│       │   │   ├── text_processor.py         # Text processing
│       │   │   ├── docx_processor.py         # Word document processing
│       │   │   └── html_processor.py         # HTML processing
│       │   ├── generators/
│       │   │   ├── __init__.py
│       │   │   ├── rag_generator.py          # RAG implementation
│       │   │   ├── mcp_generator.py          # MCP implementation
│       │   │   ├── agentic_generator.py      # Agentic workflow implementation
│       │   │   └── llm_generator.py          # Direct LLM implementation
│       │   ├── evaluators/
│       │   │   ├── __init__.py
│       │   │   ├── llm_judge.py              # LLM-as-a-judge evaluator
│       │   │   ├── retrieval_evaluator.py    # Retrieval quality evaluator
│       │   │   ├── hallucination_detector.py # Hallucination detection
│       │   │   └── relevance_scorer.py       # Relevance scoring
│       │   └── workflows/
│       │       ├── __init__.py
│       │       ├── hr_workflow.py            # HR tenant workflow
│       │       ├── coding_workflow.py        # Coding tenant workflow
│       │       ├── financial_workflow.py     # Financial analysis workflow
│       │       └── research_workflow.py      # Research workflow
│       │
│       ├── factories/                        # Factory patterns
│       │   ├── __init__.py
│       │   ├── llm_factory.py                # LLM provider factory
│       │   ├── vector_store_factory.py       # Vector store factory
│       │   ├── processor_factory.py          # Document processor factory
│       │   ├── generator_factory.py          # Generator factory
│       │   ├── evaluator_factory.py          # Evaluator factory
│       │   └── workflow_factory.py           # Workflow factory
│       │
│       ├── models/                           # Shared models
│       │   ├── __init__.py
│       │   ├── base.py                       # Base models
│       │   ├── document.py                   # Document models
│       │   ├── generation.py                 # Generation models
│       │   ├── evaluation.py                 # Evaluation models
│       │   └── workflow.py                   # Workflow models
│       │
│       ├── utils/                            # Utility functions
│       │   ├── __init__.py
│       │   ├── text_processing.py            # Text utilities
│       │   ├── embedding_utils.py            # Embedding utilities
│       │   ├── chunking.py                   # Text chunking utilities
│       │   ├── metrics.py                    # Evaluation metrics
│       │   └── async_utils.py                # Async utilities
│       │
│       └── exceptions/                       # Custom exceptions
│           ├── __init__.py
│           ├── base.py                       # Base exceptions
│           ├── llm_exceptions.py             # LLM-related exceptions
│           ├── processing_exceptions.py      # Processing exceptions
│           └── evaluation_exceptions.py      # Evaluation exceptions
│
├── tests/                                    # Test files
│   ├── __init__.py
│   ├── conftest.py                           # Test configuration
│   ├── abstractions/                        # Abstract test cases
│   ├── implementations/                      # Implementation tests
│   ├── factories/                            # Factory tests
│   └── integration/                          # Integration tests
│
├── docs/                                     # Documentation
│   ├── abstractions.md                      # Abstraction documentation
│   ├── implementations.md                   # Implementation guide
│   └── examples/                             # Usage examples
│
├── requirements.txt                          # Dependencies
├── pyproject.toml                            # Project configuration
├── Dockerfile                                # Docker configuration
└── README.md                                 # Library documentation
```

---

## 🧬 **Abstract Base Classes**

### **1. Base Abstractions**

```python
# abstractions/base.py
from abc import ABC, abstractmethod
from typing import Generic, TypeVar, Any, Dict, Optional, List
from pydantic import BaseModel
import asyncio


T = TypeVar('T')
K = TypeVar('K')
V = TypeVar('V')


class BaseConfig(BaseModel):
    """Base configuration for all components."""
    
    class Config:
        arbitrary_types_allowed = True
        extra = "forbid"


class BaseProvider(ABC, Generic[T]):
    """Base provider class for all service providers."""
    
    def __init__(self, config: BaseConfig) -> None:
        """Initialize provider with configuration.
        
        Args:
            config: Provider-specific configuration
        """
        self.config = config
        self._initialized = False
    
    @abstractmethod
    async def initialize(self) -> None:
        """Initialize the provider.
        
        Raises:
            ProviderError: If initialization fails
        """
        pass
    
    @abstractmethod
    async def health_check(self) -> bool:
        """Check provider health status.
        
        Returns:
            bool: True if healthy, False otherwise
        """
        pass
    
    @abstractmethod
    async def cleanup(self) -> None:
        """Cleanup provider resources."""
        pass
    
    async def __aenter__(self) -> 'BaseProvider[T]':
        """Async context manager entry."""
        await self.initialize()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        """Async context manager exit."""
        await self.cleanup()


class BaseProcessor(ABC, Generic[T, K]):
    """Base processor for transforming data."""
    
    @abstractmethod
    async def process(self, input_data: T) -> K:
        """Process input data and return transformed output.
        
        Args:
            input_data: Input data to process
            
        Returns:
            K: Processed output data
            
        Raises:
            ProcessingError: If processing fails
        """
        pass
    
    @abstractmethod
    async def batch_process(self, input_batch: List[T]) -> List[K]:
        """Process multiple inputs in batch.
        
        Args:
            input_batch: List of input data to process
            
        Returns:
            List[K]: List of processed outputs
            
        Raises:
            ProcessingError: If batch processing fails
        """
        pass
```

### **2. LLM Provider Abstractions**

```python
# abstractions/llm.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Union, AsyncGenerator
from pydantic import BaseModel, Field
from abstractions.base import BaseProvider, BaseConfig


class LLMMessage(BaseModel):
    """Standard message format for LLM interactions."""
    
    role: str = Field(description="Message role (system, user, assistant)")
    content: str = Field(description="Message content")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Additional metadata")


class LLMRequest(BaseModel):
    """Standard request format for LLM providers."""
    
    messages: List[LLMMessage] = Field(description="Conversation messages")
    model: str = Field(description="Model identifier")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="Generation temperature")
    max_tokens: int = Field(default=1000, ge=1, description="Maximum tokens to generate")
    stream: bool = Field(default=False, description="Enable streaming response")
    tools: Optional[List[Dict[str, Any]]] = Field(None, description="Available tools/functions")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Request metadata")


class LLMResponse(BaseModel):
    """Standard response format from LLM providers."""
    
    content: str = Field(description="Generated content")
    model: str = Field(description="Model used for generation")
    usage: Dict[str, int] = Field(description="Token usage statistics")
    finish_reason: str = Field(description="Reason for completion")
    tool_calls: Optional[List[Dict[str, Any]]] = Field(None, description="Tool calls made")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Response metadata")


class LLMConfig(BaseConfig):
    """Configuration for LLM providers."""
    
    api_key: Optional[str] = Field(None, description="API key for authentication")
    base_url: Optional[str] = Field(None, description="Base URL for API")
    timeout: int = Field(default=30, description="Request timeout in seconds")
    max_retries: int = Field(default=3, description="Maximum retry attempts")
    retry_delay: float = Field(default=1.0, description="Delay between retries")


class LLMProvider(BaseProvider[LLMResponse], ABC):
    """Abstract base class for LLM providers."""
    
    def __init__(self, config: LLMConfig) -> None:
        """Initialize LLM provider.
        
        Args:
            config: LLM provider configuration
        """
        super().__init__(config)
        self.config: LLMConfig = config
    
    @abstractmethod
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """Generate response from LLM.
        
        Args:
            request: LLM request with messages and parameters
            
        Returns:
            LLMResponse: Generated response
            
        Raises:
            LLMError: If generation fails
        """
        pass
    
    @abstractmethod
    async def stream_generate(self, request: LLMRequest) -> AsyncGenerator[str, None]:
        """Generate streaming response from LLM.
        
        Args:
            request: LLM request with streaming enabled
            
        Yields:
            str: Chunks of generated content
            
        Raises:
            LLMError: If streaming generation fails
        """
        pass
    
    @abstractmethod
    async def get_embedding(self, text: str) -> List[float]:
        """Get text embedding from LLM provider.
        
        Args:
            text: Text to embed
            
        Returns:
            List[float]: Text embedding vector
            
        Raises:
            LLMError: If embedding generation fails
        """
        pass
    
    @abstractmethod
    def get_supported_models(self) -> List[str]:
        """Get list of supported models.
        
        Returns:
            List[str]: List of supported model identifiers
        """
        pass
```

### **3. Vector Store Abstractions**

```python
# abstractions/vector_store.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Tuple
from pydantic import BaseModel, Field
from abstractions.base import BaseProvider, BaseConfig


class VectorDocument(BaseModel):
    """Document with vector embedding."""
    
    id: str = Field(description="Unique document identifier")
    content: str = Field(description="Document content")
    embedding: List[float] = Field(description="Vector embedding")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Document metadata")


class SearchResult(BaseModel):
    """Search result from vector store."""
    
    document: VectorDocument = Field(description="Retrieved document")
    score: float = Field(description="Similarity score")
    rank: int = Field(description="Result rank")


class VectorStoreConfig(BaseConfig):
    """Configuration for vector stores."""
    
    collection_name: str = Field(description="Collection/index name")
    dimension: int = Field(description="Vector dimension")
    similarity_metric: str = Field(default="cosine", description="Similarity metric")
    host: Optional[str] = Field(None, description="Vector store host")
    port: Optional[int] = Field(None, description="Vector store port")
    api_key: Optional[str] = Field(None, description="API key for authentication")


class VectorStore(BaseProvider[List[SearchResult]], ABC):
    """Abstract base class for vector stores."""
    
    def __init__(self, config: VectorStoreConfig) -> None:
        """Initialize vector store.
        
        Args:
            config: Vector store configuration
        """
        super().__init__(config)
        self.config: VectorStoreConfig = config
    
    @abstractmethod
    async def create_collection(self, collection_name: str, dimension: int) -> bool:
        """Create a new collection/index.
        
        Args:
            collection_name: Name of the collection
            dimension: Vector dimension
            
        Returns:
            bool: True if created successfully
            
        Raises:
            VectorStoreError: If creation fails
        """
        pass
    
    @abstractmethod
    async def add_documents(self, documents: List[VectorDocument]) -> bool:
        """Add documents to the vector store.
        
        Args:
            documents: List of documents with embeddings
            
        Returns:
            bool: True if added successfully
            
        Raises:
            VectorStoreError: If addition fails
        """
        pass
    
    @abstractmethod
    async def search(
        self, 
        query_embedding: List[float], 
        top_k: int = 5,
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> List[SearchResult]:
        """Search for similar documents.
        
        Args:
            query_embedding: Query vector
            top_k: Number of results to return
            filter_metadata: Metadata filters
            
        Returns:
            List[SearchResult]: Search results
            
        Raises:
            VectorStoreError: If search fails
        """
        pass
    
    @abstractmethod
    async def delete_documents(self, document_ids: List[str]) -> bool:
        """Delete documents from vector store.
        
        Args:
            document_ids: List of document IDs to delete
            
        Returns:
            bool: True if deleted successfully
            
        Raises:
            VectorStoreError: If deletion fails
        """
        pass
    
    @abstractmethod
    async def update_document(self, document: VectorDocument) -> bool:
        """Update existing document.
        
        Args:
            document: Updated document
            
        Returns:
            bool: True if updated successfully
            
        Raises:
            VectorStoreError: If update fails
        """
        pass
```

### **4. Generator Abstractions**

```python
# abstractions/generator.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field
from abstractions.base import BaseProcessor, BaseConfig
from models.generation import GenerationRequest, GenerationResponse


class GeneratorConfig(BaseConfig):
    """Configuration for generators."""
    
    tenant_type: str = Field(description="Tenant type (hr, coding, etc.)")
    task_type: str = Field(description="Task type (rag, mcp, agentic, llm)")
    llm_config: Dict[str, Any] = Field(description="LLM configuration")
    vector_store_config: Optional[Dict[str, Any]] = Field(None, description="Vector store configuration")
    custom_settings: Optional[Dict[str, Any]] = Field(None, description="Custom generator settings")


class GenerationContext(BaseModel):
    """Context for generation requests."""
    
    project_id: str = Field(description="Project identifier")
    element_id: Optional[str] = Field(None, description="Element identifier")
    user_id: str = Field(description="User identifier")
    documents: List[str] = Field(description="Available document IDs")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional context")


class Generator(BaseProcessor[GenerationRequest, GenerationResponse], ABC):
    """Abstract base class for content generators."""
    
    def __init__(self, config: GeneratorConfig) -> None:
        """Initialize generator.
        
        Args:
            config: Generator configuration
        """
        self.config = config
    
    @abstractmethod
    async def generate(
        self, 
        request: GenerationRequest, 
        context: GenerationContext
    ) -> GenerationResponse:
        """Generate content based on request and context.
        
        Args:
            request: Generation request
            context: Generation context
            
        Returns:
            GenerationResponse: Generated content
            
        Raises:
            GenerationError: If generation fails
        """
        pass
    
    @abstractmethod
    async def validate_request(self, request: GenerationRequest) -> bool:
        """Validate generation request.
        
        Args:
            request: Request to validate
            
        Returns:
            bool: True if valid
            
        Raises:
            ValidationError: If request is invalid
        """
        pass
    
    @abstractmethod
    def get_supported_features(self) -> List[str]:
        """Get list of supported features.
        
        Returns:
            List[str]: Supported features
        """
        pass
```

### **5. Evaluator Abstractions**

```python
# abstractions/evaluator.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field
from abstractions.base import BaseProcessor, BaseConfig
from models.evaluation import EvaluationRequest, EvaluationResult


class EvaluatorConfig(BaseConfig):
    """Configuration for evaluators."""
    
    evaluator_type: str = Field(description="Type of evaluator")
    llm_config: Optional[Dict[str, Any]] = Field(None, description="LLM configuration for LLM-based evaluators")
    criteria: Dict[str, float] = Field(description="Evaluation criteria and weights")
    thresholds: Dict[str, float] = Field(description="Quality thresholds")


class EvaluationContext(BaseModel):
    """Context for evaluation requests."""
    
    generation_id: str = Field(description="Generation identifier")
    project_id: str = Field(description="Project identifier")
    reference_documents: List[str] = Field(description="Reference document IDs")
    ground_truth: Optional[str] = Field(None, description="Ground truth for comparison")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional context")


class Evaluator(BaseProcessor[EvaluationRequest, EvaluationResult], ABC):
    """Abstract base class for content evaluators."""
    
    def __init__(self, config: EvaluatorConfig) -> None:
        """Initialize evaluator.
        
        Args:
            config: Evaluator configuration
        """
        self.config = config
    
    @abstractmethod
    async def evaluate(
        self, 
        request: EvaluationRequest, 
        context: EvaluationContext
    ) -> EvaluationResult:
        """Evaluate generated content.
        
        Args:
            request: Evaluation request
            context: Evaluation context
            
        Returns:
            EvaluationResult: Evaluation results
            
        Raises:
            EvaluationError: If evaluation fails
        """
        pass
    
    @abstractmethod
    async def batch_evaluate(
        self, 
        requests: List[EvaluationRequest], 
        context: EvaluationContext
    ) -> List[EvaluationResult]:
        """Evaluate multiple generations in batch.
        
        Args:
            requests: List of evaluation requests
            context: Shared evaluation context
            
        Returns:
            List[EvaluationResult]: Evaluation results
            
        Raises:
            EvaluationError: If batch evaluation fails
        """
        pass
    
    @abstractmethod
    def get_evaluation_criteria(self) -> Dict[str, str]:
        """Get evaluation criteria descriptions.
        
        Returns:
            Dict[str, str]: Criteria names and descriptions
        """
        pass
```

---

## 🏭 **Factory Patterns**

### **1. LLM Factory**

```python
# factories/llm_factory.py
from typing import Dict, Type, Any
from abstractions.llm import LLMProvider, LLMConfig
from implementations.llm import (
    OpenAIProvider, 
    AnthropicProvider, 
    OllamaProvider,
    MockProvider
)
from exceptions.base import FactoryError


class LLMFactory:
    """Factory for creating LLM provider instances."""
    
    _providers: Dict[str, Type[LLMProvider]] = {
        "openai": OpenAIProvider,
        "anthropic": AnthropicProvider,
        "ollama": OllamaProvider,
        "mock": MockProvider,
    }
    
    @classmethod
    def create_provider(cls, provider_type: str, config: Dict[str, Any]) -> LLMProvider:
        """Create LLM provider instance.
        
        Args:
            provider_type: Type of provider to create
            config: Provider configuration
            
        Returns:
            LLMProvider: Configured provider instance
            
        Raises:
            FactoryError: If provider type is not supported
        """
        if provider_type not in cls._providers:
            raise FactoryError(f"Unsupported LLM provider: {provider_type}")
        
        provider_class = cls._providers[provider_type]
        provider_config = LLMConfig(**config)
        
        return provider_class(provider_config)
    
    @classmethod
    def register_provider(cls, name: str, provider_class: Type[LLMProvider]) -> None:
        """Register a new provider type.
        
        Args:
            name: Provider name
            provider_class: Provider class
        """
        cls._providers[name] = provider_class
    
    @classmethod
    def get_supported_providers(cls) -> List[str]:
        """Get list of supported provider types.
        
        Returns:
            List[str]: Supported provider names
        """
        return list(cls._providers.keys())
```

### **2. Generator Factory**

```python
# factories/generator_factory.py
from typing import Dict, Type, Any
from abstractions.generator import Generator, GeneratorConfig
from implementations.generators import (
    RAGGenerator,
    MCPGenerator, 
    AgenticGenerator,
    LLMGenerator
)
from models.enums import TenantType, TaskType, TENANT_TASK_MAPPING
from exceptions.base import FactoryError


class GeneratorFactory:
    """Factory for creating generator instances based on tenant and task types."""
    
    _generators: Dict[TaskType, Type[Generator]] = {
        TaskType.RAG: RAGGenerator,
        TaskType.MCP: MCPGenerator,
        TaskType.AGENTIC_WORKFLOW: AgenticGenerator,
        TaskType.LLM: LLMGenerator,
    }
    
    @classmethod
    def create_generator(
        cls, 
        tenant_type: TenantType, 
        config: Dict[str, Any]
    ) -> Generator:
        """Create generator based on tenant type.
        
        Args:
            tenant_type: Tenant type determining the task type
            config: Generator configuration
            
        Returns:
            Generator: Configured generator instance
            
        Raises:
            FactoryError: If tenant type is not supported
        """
        task_type = TENANT_TASK_MAPPING.get(tenant_type)
        if not task_type:
            raise FactoryError(f"Unsupported tenant type: {tenant_type}")
        
        if task_type not in cls._generators:
            raise FactoryError(f"No generator available for task type: {task_type}")
        
        generator_class = cls._generators[task_type]
        generator_config = GeneratorConfig(
            tenant_type=tenant_type.value,
            task_type=task_type.value,
            **config
        )
        
        return generator_class(generator_config)
    
    @classmethod
    def create_generator_by_task_type(
        cls, 
        task_type: TaskType, 
        config: Dict[str, Any]
    ) -> Generator:
        """Create generator directly by task type.
        
        Args:
            task_type: Task type
            config: Generator configuration
            
        Returns:
            Generator: Configured generator instance
            
        Raises:
            FactoryError: If task type is not supported
        """
        if task_type not in cls._generators:
            raise FactoryError(f"No generator available for task type: {task_type}")
        
        generator_class = cls._generators[task_type]
        generator_config = GeneratorConfig(
            tenant_type="unknown",  # Will be set by caller
            task_type=task_type.value,
            **config
        )
        
        return generator_class(generator_config)
    
    @classmethod
    def register_generator(cls, task_type: TaskType, generator_class: Type[Generator]) -> None:
        """Register a new generator type.
        
        Args:
            task_type: Task type
            generator_class: Generator class
        """
        cls._generators[task_type] = generator_class
```

---

## 🔧 **Concrete Implementations**

### **1. OpenAI Provider Implementation**

```python
# implementations/llm/openai_provider.py
import asyncio
import openai
from typing import List, AsyncGenerator, Optional
from abstractions.llm import LLMProvider, LLMRequest, LLMResponse, LLMConfig, LLMMessage
from exceptions.llm_exceptions import LLMError, LLMTimeoutError, LLMQuotaError


class OpenAIConfig(LLMConfig):
    """OpenAI-specific configuration."""
    
    organization: Optional[str] = None
    project: Optional[str] = None


class OpenAIProvider(LLMProvider):
    """OpenAI LLM provider implementation."""
    
    def __init__(self, config: OpenAIConfig) -> None:
        """Initialize OpenAI provider.
        
        Args:
            config: OpenAI configuration
        """
        super().__init__(config)
        self.config: OpenAIConfig = config
        self.client: Optional[openai.AsyncOpenAI] = None
    
    async def initialize(self) -> None:
        """Initialize OpenAI client."""
        try:
            self.client = openai.AsyncOpenAI(
                api_key=self.config.api_key,
                organization=self.config.organization,
                project=self.config.project,
                base_url=self.config.base_url,
                timeout=self.config.timeout
            )
            self._initialized = True
        except Exception as e:
            raise LLMError(f"Failed to initialize OpenAI client: {str(e)}")
    
    async def health_check(self) -> bool:
        """Check OpenAI API health."""
        try:
            if not self.client:
                return False
            
            # Simple API test
            await self.client.models.list()
            return True
        except Exception:
            return False
    
    async def cleanup(self) -> None:
        """Cleanup OpenAI client."""
        if self.client:
            await self.client.close()
            self.client = None
    
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """Generate response using OpenAI API.
        
        Args:
            request: LLM request
            
        Returns:
            LLMResponse: Generated response
            
        Raises:
            LLMError: If generation fails
        """
        if not self.client:
            raise LLMError("OpenAI client not initialized")
        
        try:
            # Convert internal format to OpenAI format
            messages = [
                {"role": msg.role, "content": msg.content}
                for msg in request.messages
            ]
            
            response = await self.client.chat.completions.create(
                model=request.model,
                messages=messages,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
                tools=request.tools,
                timeout=self.config.timeout
            )
            
            # Convert OpenAI response to internal format
            return LLMResponse(
                content=response.choices[0].message.content or "",
                model=response.model,
                usage={
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                },
                finish_reason=response.choices[0].finish_reason,
                tool_calls=response.choices[0].message.tool_calls,
                metadata={"response_id": response.id}
            )
            
        except openai.RateLimitError as e:
            raise LLMQuotaError(f"OpenAI rate limit exceeded: {str(e)}")
        except openai.APITimeoutError as e:
            raise LLMTimeoutError(f"OpenAI request timeout: {str(e)}")
        except Exception as e:
            raise LLMError(f"OpenAI generation failed: {str(e)}")
    
    async def stream_generate(self, request: LLMRequest) -> AsyncGenerator[str, None]:
        """Generate streaming response using OpenAI API.
        
        Args:
            request: LLM request with streaming
            
        Yields:
            str: Content chunks
            
        Raises:
            LLMError: If streaming fails
        """
        if not self.client:
            raise LLMError("OpenAI client not initialized")
        
        try:
            messages = [
                {"role": msg.role, "content": msg.content}
                for msg in request.messages
            ]
            
            stream = await self.client.chat.completions.create(
                model=request.model,
                messages=messages,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
                stream=True,
                timeout=self.config.timeout
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            raise LLMError(f"OpenAI streaming failed: {str(e)}")
    
    async def get_embedding(self, text: str) -> List[float]:
        """Get text embedding from OpenAI.
        
        Args:
            text: Text to embed
            
        Returns:
            List[float]: Embedding vector
            
        Raises:
            LLMError: If embedding fails
        """
        if not self.client:
            raise LLMError("OpenAI client not initialized")
        
        try:
            response = await self.client.embeddings.create(
                model="text-embedding-3-small",
                input=text,
                timeout=self.config.timeout
            )
            
            return response.data[0].embedding
            
        except Exception as e:
            raise LLMError(f"OpenAI embedding failed: {str(e)}")
    
    def get_supported_models(self) -> List[str]:
        """Get supported OpenAI models.
        
        Returns:
            List[str]: Supported model names
        """
        return [
            "gpt-4o",
            "gpt-4o-mini", 
            "gpt-4-turbo",
            "gpt-3.5-turbo",
            "gpt-4"
        ]
```

### **2. RAG Generator Implementation**

```python
# implementations/generators/rag_generator.py
from typing import List, Dict, Any, Optional
from abstractions.generator import Generator, GeneratorConfig, GenerationContext
from abstractions.llm import LLMProvider
from abstractions.vector_store import VectorStore
from factories.llm_factory import LLMFactory
from factories.vector_store_factory import VectorStoreFactory
from models.generation import GenerationRequest, GenerationResponse
from utils.text_processing import create_context_from_documents
from exceptions.base import GenerationError


class RAGGenerator(Generator):
    """RAG (Retrieval Augmented Generation) implementation."""
    
    def __init__(self, config: GeneratorConfig) -> None:
        """Initialize RAG generator.
        
        Args:
            config: Generator configuration
        """
        super().__init__(config)
        self.llm_provider: Optional[LLMProvider] = None
        self.vector_store: Optional[VectorStore] = None
    
    async def initialize(self) -> None:
        """Initialize RAG components."""
        try:
            # Initialize LLM provider
            llm_config = self.config.llm_config
            self.llm_provider = LLMFactory.create_provider(
                llm_config["provider_type"],
                llm_config
            )
            await self.llm_provider.initialize()
            
            # Initialize vector store if configured
            if self.config.vector_store_config:
                vs_config = self.config.vector_store_config
                self.vector_store = VectorStoreFactory.create_store(
                    vs_config["store_type"],
                    vs_config
                )
                await self.vector_store.initialize()
                
        except Exception as e:
            raise GenerationError(f"Failed to initialize RAG generator: {str(e)}")
    
    async def generate(
        self, 
        request: GenerationRequest, 
        context: GenerationContext
    ) -> GenerationResponse:
        """Generate response using RAG approach.
        
        Args:
            request: Generation request
            context: Generation context
            
        Returns:
            GenerationResponse: Generated response
            
        Raises:
            GenerationError: If generation fails
        """
        try:
            # Step 1: Retrieve relevant documents
            relevant_docs = await self._retrieve_documents(
                request.query, 
                context.documents,
                top_k=5
            )
            
            # Step 2: Create augmented context
            augmented_context = await self._create_augmented_context(
                request.query,
                relevant_docs,
                context
            )
            
            # Step 3: Generate response
            llm_request = await self._create_llm_request(
                request.query,
                augmented_context,
                request.parameters
            )
            
            llm_response = await self.llm_provider.generate(llm_request)
            
            # Step 4: Create generation response
            return GenerationResponse(
                content=llm_response.content,
                metadata={
                    "model_used": llm_response.model,
                    "token_usage": llm_response.usage,
                    "documents_used": [doc["id"] for doc in relevant_docs],
                    "retrieval_scores": [doc["score"] for doc in relevant_docs],
                    "generation_type": "rag"
                },
                citations=self._extract_citations(relevant_docs),
                finish_reason=llm_response.finish_reason
            )
            
        except Exception as e:
            raise GenerationError(f"RAG generation failed: {str(e)}")
    
    async def _retrieve_documents(
        self, 
        query: str, 
        available_docs: List[str],
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """Retrieve relevant documents for the query.
        
        Args:
            query: Search query
            available_docs: Available document IDs
            top_k: Number of documents to retrieve
            
        Returns:
            List[Dict[str, Any]]: Retrieved documents with scores
        """
        if not self.vector_store:
            # Fallback: return all available documents
            return [{"id": doc_id, "score": 1.0} for doc_id in available_docs[:top_k]]
        
        # Get query embedding
        query_embedding = await self.llm_provider.get_embedding(query)
        
        # Search vector store
        search_results = await self.vector_store.search(
            query_embedding, 
            top_k=top_k,
            filter_metadata={"doc_id": {"$in": available_docs}}
        )
        
        return [
            {
                "id": result.document.id,
                "content": result.document.content,
                "score": result.score,
                "metadata": result.document.metadata
            }
            for result in search_results
        ]
    
    async def validate_request(self, request: GenerationRequest) -> bool:
        """Validate RAG generation request.
        
        Args:
            request: Request to validate
            
        Returns:
            bool: True if valid
        """
        if not request.query or not request.query.strip():
            return False
        
        return True
    
    def get_supported_features(self) -> List[str]:
        """Get supported RAG features.
        
        Returns:
            List[str]: Supported features
        """
        return [
            "document_retrieval",
            "semantic_search", 
            "context_augmentation",
            "citation_generation",
            "relevance_scoring"
        ]
```

---

## 📊 **Implementation Progress**

### **✅ Completed Design Elements**
- [x] 🏗️ Abstract base classes for all core components
- [x] 🧬 LLM provider abstractions with full async support
- [x] 📦 Vector store abstractions with search capabilities
- [x] 🎯 Generator abstractions for different task types
- [x] 📊 Evaluator abstractions for quality assessment
- [x] 🏭 Factory patterns for component creation
- [x] 🔧 Concrete implementation examples

### **🔄 Next Implementation Steps**
- [ ] 📝 Complete all concrete implementations
- [ ] 🧪 Abstract test cases for all components
- [ ] 📚 Documentation and usage examples
- [ ] 🔗 Integration with main API
- [ ] 🚀 Performance optimization

---

## 🎯 **Usage Examples**

### **Creating a RAG Generator**

```python
# Example: Creating and using a RAG generator
from factories.generator_factory import GeneratorFactory
from models.enums import TenantType
from models.generation import GenerationRequest, GenerationContext

# Create generator for HR tenant
generator = GeneratorFactory.create_generator(
    tenant_type=TenantType.HR,
    config={
        "llm_config": {
            "provider_type": "openai",
            "api_key": "your-api-key",
            "model": "gpt-4o-mini"
        },
        "vector_store_config": {
            "store_type": "qdrant", 
            "host": "localhost",
            "port": 6333,
            "collection_name": "hr_documents"
        }
    }
)

# Initialize generator
await generator.initialize()

# Create generation request
request = GenerationRequest(
    query="What is our company's vacation policy?",
    parameters={"temperature": 0.3, "max_tokens": 500}
)

context = GenerationContext(
    project_id="project_123",
    user_id="user_456", 
    documents=["doc_1", "doc_2", "doc_3"],
    metadata={"department": "hr"}
)

# Generate response
response = await generator.generate(request, context)
print(f"Generated: {response.content}")
print(f"Sources: {response.citations}")
```

---

*This comprehensive abstraction design provides a solid foundation for extending TinyRAG v1.4 with pluggable components, tenant-specific implementations, and maintainable architecture.* 