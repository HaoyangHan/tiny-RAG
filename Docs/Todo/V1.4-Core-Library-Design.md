# TinyRAG v1.4 Core Library Design

📅 **Date:** 2025-06-24  
🎯 **Version:** 1.4.0  
👨‍💻 **Developer:** AI Assistant  
📝 **Objective:** Design comprehensive core library with abstractions and concrete implementations

---

## 🧬 **Abstract Base Classes**

### **1. LLM Provider Abstraction**

```python
# rag-memo-core-lib/src/rag_memo_core_lib/abstractions/llm.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, AsyncGenerator
from pydantic import BaseModel, Field


class LLMMessage(BaseModel):
    """Standard message format for LLM interactions."""
    
    role: str = Field(description="Message role (system, user, assistant)")
    content: str = Field(description="Message content")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Additional metadata")


class LLMRequest(BaseModel):
    """Standard request format for LLM providers."""
    
    messages: List[LLMMessage] = Field(description="Conversation messages")
    model: str = Field(description="Model identifier")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0)
    max_tokens: int = Field(default=1000, ge=1)
    stream: bool = Field(default=False)
    tools: Optional[List[Dict[str, Any]]] = Field(None)


class LLMResponse(BaseModel):
    """Standard response format from LLM providers."""
    
    content: str = Field(description="Generated content")
    model: str = Field(description="Model used")
    usage: Dict[str, int] = Field(description="Token usage")
    finish_reason: str = Field(description="Completion reason")
    tool_calls: Optional[List[Dict[str, Any]]] = Field(None)


class LLMProvider(ABC):
    """Abstract base class for LLM providers."""
    
    @abstractmethod
    async def initialize(self) -> None:
        """Initialize the LLM provider."""
        pass
    
    @abstractmethod
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """Generate response from LLM."""
        pass
    
    @abstractmethod
    async def stream_generate(self, request: LLMRequest) -> AsyncGenerator[str, None]:
        """Generate streaming response."""
        pass
    
    @abstractmethod
    async def get_embedding(self, text: str) -> List[float]:
        """Get text embedding."""
        pass
    
    @abstractmethod
    def get_supported_models(self) -> List[str]:
        """Get supported models."""
        pass
```

### **2. Generator Abstraction**

```python
# rag-memo-core-lib/src/rag_memo_core_lib/abstractions/generator.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field


class GenerationRequest(BaseModel):
    """Request for content generation."""
    
    query: str = Field(description="Input query")
    context_documents: List[str] = Field(description="Document IDs for context")
    parameters: Optional[Dict[str, Any]] = Field(None, description="Generation parameters")


class GenerationResponse(BaseModel):
    """Response from content generation."""
    
    content: str = Field(description="Generated content")
    metadata: Dict[str, Any] = Field(description="Generation metadata")
    citations: List[Dict[str, Any]] = Field(description="Source citations")
    finish_reason: str = Field(description="Completion reason")


class GenerationContext(BaseModel):
    """Context for generation."""
    
    project_id: str = Field(description="Project ID")
    element_id: Optional[str] = Field(None, description="Element ID")
    user_id: str = Field(description="User ID")
    tenant_type: str = Field(description="Tenant type")
    task_type: str = Field(description="Task type")


class Generator(ABC):
    """Abstract base class for content generators."""
    
    @abstractmethod
    async def generate(
        self, 
        request: GenerationRequest, 
        context: GenerationContext
    ) -> GenerationResponse:
        """Generate content based on request and context."""
        pass
    
    @abstractmethod
    async def validate_request(self, request: GenerationRequest) -> bool:
        """Validate generation request."""
        pass
    
    @abstractmethod
    def get_supported_features(self) -> List[str]:
        """Get supported features."""
        pass
```

### **3. Evaluator Abstraction**

```python
# rag-memo-core-lib/src/rag_memo_core_lib/abstractions/evaluator.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field


class EvaluationRequest(BaseModel):
    """Request for content evaluation."""
    
    generated_content: str = Field(description="Content to evaluate")
    query: str = Field(description="Original query")
    context_documents: List[str] = Field(description="Context documents")
    criteria: Dict[str, float] = Field(description="Evaluation criteria weights")


class EvaluationResult(BaseModel):
    """Result of content evaluation."""
    
    overall_score: float = Field(ge=0, le=1, description="Overall score")
    relevance_score: float = Field(ge=0, le=1, description="Relevance score")
    accuracy_score: float = Field(ge=0, le=1, description="Accuracy score")
    completeness_score: float = Field(ge=0, le=1, description="Completeness score")
    clarity_score: float = Field(ge=0, le=1, description="Clarity score")
    hallucination_detected: bool = Field(description="Hallucination detection")
    strengths: List[str] = Field(description="Identified strengths")
    weaknesses: List[str] = Field(description="Identified weaknesses")
    suggestions: List[str] = Field(description="Improvement suggestions")


class Evaluator(ABC):
    """Abstract base class for content evaluators."""
    
    @abstractmethod
    async def evaluate(
        self, 
        request: EvaluationRequest
    ) -> EvaluationResult:
        """Evaluate generated content."""
        pass
    
    @abstractmethod
    def get_evaluation_criteria(self) -> Dict[str, str]:
        """Get evaluation criteria descriptions."""
        pass
```

---

## 🏭 **Factory Patterns**

### **1. LLM Factory Integration**

```python
# rag-memo-core-lib/src/rag_memo_core_lib/factories/llm_factory.py
from typing import Dict, Type, Any, List
from abstractions.llm import LLMProvider
from implementations.llm import FactoryLLMProvider
from services.llm_factory import llm_factory as existing_factory


class CoreLLMFactory:
    """Factory for creating LLM provider instances using existing llm_factory."""
    
    def __init__(self):
        """Initialize with existing factory reference."""
        self.existing_factory = existing_factory
    
    @classmethod
    def create_provider(cls, provider_type: str = "factory", config: Dict[str, Any] = None) -> LLMProvider:
        """Create LLM provider instance using existing factory.
        
        Args:
            provider_type: Always "factory" (uses existing llm_factory)
            config: Provider configuration
            
        Returns:
            LLMProvider: Configured provider
        """
        if config is None:
            config = {}
        
        return FactoryLLMProvider(config)
    
    @classmethod
    def get_supported_providers(cls) -> List[str]:
        """Get supported provider types."""
        return ["factory"]
    
    @classmethod
    def get_supported_models(cls) -> Dict[str, List[str]]:
        """Get supported models from existing factory."""
        return existing_factory.get_available_models()
```

### **2. Embedding Factory**

```python
# rag-memo-core-lib/src/rag_memo_core_lib/factories/embedding_factory.py
import os
import logging
from typing import List, Dict, Any
from enum import Enum
from abc import ABC, abstractmethod
import openai
from pydantic import BaseModel

logger = logging.getLogger(__name__)


class EmbeddingModel(str, Enum):
    """Supported embedding models."""
    TEXT_EMBEDDING_ADA_002 = "text-embedding-ada-002"
    TEXT_EMBEDDING_3_SMALL = "text-embedding-3-small"


class EmbeddingProvider(str, Enum):
    """Supported embedding providers."""
    OPENAI = "openai"


class EmbeddingResponse(BaseModel):
    """Standard response format for embeddings."""
    embedding: List[float]
    model: str
    provider: str
    usage: Dict[str, Any]


class BaseEmbedding(ABC):
    """Abstract base class for embedding implementations."""
    
    def __init__(self, model: str, api_key: str):
        self.model = model
        self.api_key = api_key
    
    @abstractmethod
    async def get_embedding(self, text: str) -> EmbeddingResponse:
        """Get embedding for text."""
        pass


class OpenAIEmbedding(BaseEmbedding):
    """OpenAI embedding implementation."""
    
    def __init__(self, model: str, api_key: str):
        super().__init__(model, api_key)
        self.client = openai.OpenAI(
            base_url='https://api.openai-proxy.org/v1',
            api_key=api_key,
        )
    
    async def get_embedding(self, text: str) -> EmbeddingResponse:
        """Get embedding using OpenAI API."""
        try:
            response = self.client.embeddings.create(
                model=self.model,
                input=text
            )
            
            return EmbeddingResponse(
                embedding=response.data[0].embedding,
                model=self.model,
                provider=EmbeddingProvider.OPENAI,
                usage={
                    "prompt_tokens": response.usage.prompt_tokens,
                    "total_tokens": response.usage.total_tokens,
                }
            )
            
        except Exception as e:
            logger.error(f"Error generating embedding with OpenAI: {str(e)}")
            raise


class CoreEmbeddingFactory:
    """Factory class for creating and managing embedding instances."""
    
    def __init__(self):
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.default_model = EmbeddingModel.TEXT_EMBEDDING_3_SMALL
        
        # Model to provider mapping
        self.model_providers = {
            EmbeddingModel.TEXT_EMBEDDING_ADA_002: EmbeddingProvider.OPENAI,
            EmbeddingModel.TEXT_EMBEDDING_3_SMALL: EmbeddingProvider.OPENAI,
        }
    
    def create_embedding_client(self, model: str = None) -> BaseEmbedding:
        """Create an embedding client for the specified model."""
        if not model:
            model = self.default_model
        
        if model not in self.model_providers:
            raise ValueError(f"Unsupported embedding model: {model}")
        
        provider = self.model_providers[model]
        
        if provider == EmbeddingProvider.OPENAI:
            if not self.openai_api_key:
                raise ValueError("OpenAI API key not configured")
            return OpenAIEmbedding(model, self.openai_api_key)
        else:
            raise ValueError(f"Unsupported provider: {provider}")
    
    async def get_embedding(self, text: str, model: str = None) -> List[float]:
        """Get embedding for text using specified model."""
        client = self.create_embedding_client(model)
        response = await client.get_embedding(text)
        return response.embedding
    
    def get_available_models(self) -> List[str]:
        """Get list of available embedding models."""
        return list(self.model_providers.keys())


# Global embedding factory instance
core_embedding_factory = CoreEmbeddingFactory()
```

### **2. Generator Factory**

```python
# rag-memo-core-lib/src/rag_memo_core_lib/factories/generator_factory.py
from typing import Dict, Type, Any
from abstractions.generator import Generator
from implementations.generators import RAGGenerator, MCPGenerator, AgenticGenerator, LLMGenerator
from models.enums import TenantType, TaskType, TENANT_TASK_MAPPING


class GeneratorFactory:
    """Factory for creating generator instances."""
    
    _generators: Dict[TaskType, Type[Generator]] = {
        TaskType.RAG: RAGGenerator,
        TaskType.MCP: MCPGenerator,
        TaskType.AGENTIC_WORKFLOW: AgenticGenerator,
        TaskType.LLM: LLMGenerator,
    }
    
    @classmethod
    def create_generator(
        cls, 
        tenant_type: TenantType, 
        config: Dict[str, Any]
    ) -> Generator:
        """Create generator based on tenant type.
        
        Args:
            tenant_type: Tenant type
            config: Generator configuration
            
        Returns:
            Generator: Configured generator
            
        Raises:
            ValueError: If tenant type not supported
        """
        task_type = TENANT_TASK_MAPPING.get(tenant_type)
        if not task_type:
            raise ValueError(f"Unsupported tenant type: {tenant_type}")
        
        if task_type not in cls._generators:
            raise ValueError(f"No generator for task type: {task_type}")
        
        generator_class = cls._generators[task_type]
        return generator_class(config)
```

---

## 🔧 **Concrete Implementations**

### **1. Factory LLM Provider**

```python
# rag-memo-core-lib/src/rag_memo_core_lib/implementations/llm/factory_provider.py
from typing import List, AsyncGenerator, Dict, Any
from abstractions.llm import LLMProvider, LLMRequest, LLMResponse
from services.llm_factory import llm_factory, LLMMessage as FactoryMessage


class FactoryLLMProvider(LLMProvider):
    """LLM provider implementation using existing llm_factory."""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize factory-based provider."""
        self.config = config
        self.model = config.get("model", "gpt-4o-mini")
    
    async def initialize(self) -> None:
        """Initialize the provider (factory handles initialization)."""
        # Factory is already initialized globally
        pass
    
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """Generate response using llm_factory."""
        try:
            # Convert to factory format
            factory_messages = [
                FactoryMessage(role=msg.role, content=msg.content)
                for msg in request.messages
            ]
            
            # Use factory for generation
            response = await llm_factory.generate_response(
                messages=factory_messages,
                model=request.model or self.model,
                temperature=request.temperature,
                max_tokens=request.max_tokens
            )
            
            return LLMResponse(
                content=response.content,
                model=response.model,
                usage=response.usage or {},
                finish_reason="stop",
                tool_calls=None
            )
            
        except Exception as e:
            raise RuntimeError(f"LLM generation failed: {str(e)}")
    
    async def stream_generate(self, request: LLMRequest) -> AsyncGenerator[str, None]:
        """Generate streaming response (not implemented in factory yet)."""
        # For now, return non-streaming response as single chunk
        response = await self.generate(request)
        yield response.content
    
    async def get_embedding(self, text: str, model: str = "text-embedding-3-small") -> List[float]:
        """Get text embedding using embedding factory."""
        try:
            from factories.embedding_factory import core_embedding_factory
            return await core_embedding_factory.get_embedding(text, model)
        except Exception as e:
            raise RuntimeError(f"Embedding generation failed: {str(e)}")
    
    def get_supported_models(self) -> List[str]:
        """Get supported models from factory."""
        models_by_provider = llm_factory.get_available_models()
        all_models = []
        for provider_models in models_by_provider.values():
            all_models.extend(provider_models)
        return all_models
```

### **2. RAG Generator with LlamaIndex**

```python
# rag-memo-core-lib/src/rag_memo_core_lib/implementations/generators/rag_generator.py
from typing import List, Dict, Any
from abstractions.generator import Generator, GenerationRequest, GenerationResponse, GenerationContext
from abstractions.llm import LLMProvider, LLMRequest, LLMMessage
from factories.llm_factory import CoreLLMFactory
from factories.embedding_factory import core_embedding_factory

# LlamaIndex imports
from llama_index.core import VectorStoreIndex, StorageContext, Document
from llama_index.core.llms import LLM
from llama_index.core.embeddings import BaseEmbedding
from llama_index.core.vector_stores import VectorStore as LlamaVectorStore
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever
import qdrant_client


class LlamaIndexRAGGenerator(Generator):
    """RAG (Retrieval Augmented Generation) implementation using LlamaIndex."""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize RAG generator with LlamaIndex."""
        self.config = config
        self.llm_provider: LLMProvider = None
        self.vector_store: LlamaVectorStore = None
        self.index: VectorStoreIndex = None
        self.query_engine = None
    
    async def initialize(self) -> None:
        """Initialize RAG components with LlamaIndex."""
        # Initialize LLM provider
        llm_config = self.config.get("llm_config", {})
        self.llm_provider = CoreLLMFactory.create_provider("factory", llm_config)
        await self.llm_provider.initialize()
        
        # Initialize LlamaIndex vector store (Qdrant)
        if "qdrant_config" in self.config:
            qdrant_config = self.config["qdrant_config"]
            client = qdrant_client.QdrantClient(
                host=qdrant_config.get("host", "localhost"),
                port=qdrant_config.get("port", 6333)
            )
            
            self.vector_store = QdrantVectorStore(
                client=client,
                collection_name=qdrant_config.get("collection_name", "documents")
            )
            
            # Create storage context
            storage_context = StorageContext.from_defaults(vector_store=self.vector_store)
            
            # Create index
            self.index = VectorStoreIndex(
                [],  # Empty initially
                storage_context=storage_context
            )
            
            # Create query engine
            self.query_engine = RetrieverQueryEngine.from_args(
                retriever=VectorIndexRetriever(
                    index=self.index,
                    similarity_top_k=5
                )
            )
    
    async def generate(
        self, 
        request: GenerationRequest, 
        context: GenerationContext
    ) -> GenerationResponse:
        """Generate response using RAG approach."""
        try:
            # Step 1: Retrieve relevant documents
            relevant_docs = await self._retrieve_documents(
                request.query, 
                request.context_documents,
                top_k=5
            )
            
            # Step 2: Create augmented context
            augmented_context = self._create_augmented_context(
                request.query,
                relevant_docs
            )
            
            # Step 3: Generate response
            llm_request = LLMRequest(
                messages=[
                    LLMMessage(
                        role="system",
                        content="You are a helpful assistant. Use the provided context to answer questions accurately."
                    ),
                    LLMMessage(
                        role="user",
                        content=f"Context:\n{augmented_context}\n\nQuestion: {request.query}"
                    )
                ],
                model=self.config["llm_config"]["model"],
                temperature=request.parameters.get("temperature", 0.7),
                max_tokens=request.parameters.get("max_tokens", 1000)
            )
            
            llm_response = await self.llm_provider.generate(llm_request)
            
            return GenerationResponse(
                content=llm_response.content,
                metadata={
                    "model_used": llm_response.model,
                    "token_usage": llm_response.usage,
                    "documents_used": [doc["id"] for doc in relevant_docs],
                    "retrieval_scores": [doc["score"] for doc in relevant_docs],
                    "generation_type": "rag"
                },
                citations=self._extract_citations(relevant_docs),
                finish_reason=llm_response.finish_reason
            )
            
        except Exception as e:
            raise RuntimeError(f"RAG generation failed: {str(e)}")
    
    async def _retrieve_documents(
        self, 
        query: str, 
        available_docs: List[str],
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """Retrieve relevant documents."""
        if not self.vector_store:
            # Fallback: return all available documents
            return [{"id": doc_id, "score": 1.0, "content": f"Document {doc_id}"} 
                   for doc_id in available_docs[:top_k]]
        
        # Get query embedding
        query_embedding = await self.llm_provider.get_embedding(query)
        
        # Search vector store
        search_results = await self.vector_store.search(
            query_embedding, 
            top_k=top_k,
            filter_metadata={"doc_id": {"$in": available_docs}}
        )
        
        return [
            {
                "id": result.document.id,
                "content": result.document.content,
                "score": result.score,
                "metadata": result.document.metadata
            }
            for result in search_results
        ]
    
    def _create_augmented_context(
        self, 
        query: str, 
        documents: List[Dict[str, Any]]
    ) -> str:
        """Create augmented context from retrieved documents."""
        context_parts = []
        for i, doc in enumerate(documents, 1):
            context_parts.append(f"[Document {i}]\n{doc['content']}\n")
        
        return "\n".join(context_parts)
    
    def _extract_citations(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Extract citations from documents."""
        return [
            {
                "document_id": doc["id"],
                "relevance_score": doc["score"],
                "title": doc.get("metadata", {}).get("title", f"Document {doc['id']}")
            }
            for doc in documents
        ]
    
    async def validate_request(self, request: GenerationRequest) -> bool:
        """Validate RAG generation request."""
        if not request.query or not request.query.strip():
            return False
        return True
    
    def get_supported_features(self) -> List[str]:
        """Get supported RAG features."""
        return [
            "document_retrieval",
            "semantic_search",
            "context_augmentation",
            "citation_generation",
            "relevance_scoring"
        ]
```

### **3. LLM-as-a-Judge Evaluator**

```python
# rag-memo-core-lib/src/rag_memo_core_lib/implementations/evaluators/llm_judge.py
from typing import Dict, List
from abstractions.evaluator import Evaluator, EvaluationRequest, EvaluationResult
from abstractions.llm import LLMProvider, LLMRequest, LLMMessage
from factories.llm_factory import LLMFactory
import json


class LLMJudgeEvaluator(Evaluator):
    """LLM-as-a-judge evaluator implementation."""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize LLM judge evaluator."""
        self.config = config
        self.llm_provider: LLMProvider = None
    
    async def initialize(self) -> None:
        """Initialize evaluator."""
        llm_config = self.config["llm_config"]
        self.llm_provider = LLMFactory.create_provider(
            llm_config["provider_type"],
            llm_config
        )
        await self.llm_provider.initialize()
    
    async def evaluate(self, request: EvaluationRequest) -> EvaluationResult:
        """Evaluate content using LLM-as-a-judge."""
        try:
            # Create evaluation prompt
            evaluation_prompt = self._create_evaluation_prompt(request)
            
            # Get LLM evaluation
            llm_request = LLMRequest(
                messages=[
                    LLMMessage(
                        role="system",
                        content="You are an expert evaluator. Analyze the given content and provide detailed scores."
                    ),
                    LLMMessage(
                        role="user",
                        content=evaluation_prompt
                    )
                ],
                model=self.config["llm_config"]["model"],
                temperature=0.1,  # Low temperature for consistent evaluation
                max_tokens=1000
            )
            
            llm_response = await self.llm_provider.generate(llm_request)
            
            # Parse evaluation results
            return self._parse_evaluation_response(llm_response.content)
            
        except Exception as e:
            raise RuntimeError(f"LLM evaluation failed: {str(e)}")
    
    def _create_evaluation_prompt(self, request: EvaluationRequest) -> str:
        """Create evaluation prompt."""
        context_docs = "\n".join([f"Document {i+1}: {doc}" 
                                 for i, doc in enumerate(request.context_documents)])
        
        prompt = f"""
Please evaluate the following generated content based on these criteria:

**Original Query:** {request.query}

**Context Documents:**
{context_docs}

**Generated Content:**
{request.generated_content}

**Evaluation Criteria:**
1. Relevance (0-1): How well does the content address the query?
2. Accuracy (0-1): How factually correct is the content?
3. Completeness (0-1): How complete is the answer?
4. Clarity (0-1): How clear and well-structured is the content?

**Instructions:**
- Provide scores for each criterion (0.0 to 1.0)
- Identify if any hallucinations are present
- List 2-3 strengths of the content
- List 2-3 areas for improvement
- Provide 2-3 specific suggestions

**Response Format (JSON):**
{{
    "relevance_score": 0.0,
    "accuracy_score": 0.0,
    "completeness_score": 0.0,
    "clarity_score": 0.0,
    "hallucination_detected": false,
    "strengths": ["strength1", "strength2"],
    "weaknesses": ["weakness1", "weakness2"],
    "suggestions": ["suggestion1", "suggestion2"]
}}
"""
        return prompt
    
    def _parse_evaluation_response(self, response: str) -> EvaluationResult:
        """Parse LLM evaluation response."""
        try:
            # Extract JSON from response
            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1
            json_str = response[start_idx:end_idx]
            
            evaluation_data = json.loads(json_str)
            
            # Calculate overall score
            overall_score = (
                evaluation_data["relevance_score"] * 0.3 +
                evaluation_data["accuracy_score"] * 0.3 +
                evaluation_data["completeness_score"] * 0.2 +
                evaluation_data["clarity_score"] * 0.2
            )
            
            return EvaluationResult(
                overall_score=overall_score,
                relevance_score=evaluation_data["relevance_score"],
                accuracy_score=evaluation_data["accuracy_score"],
                completeness_score=evaluation_data["completeness_score"],
                clarity_score=evaluation_data["clarity_score"],
                hallucination_detected=evaluation_data["hallucination_detected"],
                strengths=evaluation_data["strengths"],
                weaknesses=evaluation_data["weaknesses"],
                suggestions=evaluation_data["suggestions"]
            )
            
        except Exception as e:
            # Fallback if parsing fails
            return EvaluationResult(
                overall_score=0.5,
                relevance_score=0.5,
                accuracy_score=0.5,
                completeness_score=0.5,
                clarity_score=0.5,
                hallucination_detected=False,
                strengths=["Unable to parse detailed evaluation"],
                weaknesses=["Evaluation parsing failed"],
                suggestions=["Retry evaluation with different prompt"]
            )
    
    def get_evaluation_criteria(self) -> Dict[str, str]:
        """Get evaluation criteria descriptions."""
        return {
            "relevance": "How well the content addresses the original query",
            "accuracy": "Factual correctness of the information provided",
            "completeness": "How comprehensive the answer is",
            "clarity": "How clear and well-structured the content is",
            "hallucination": "Whether the content contains false or unsupported claims"
        }
```

---

## 📊 **Usage Examples**

### **1. Creating and Using a RAG Generator**

```python
# Example: Using the RAG generator
from factories.generator_factory import GeneratorFactory
from models.enums import TenantType
from abstractions.generator import GenerationRequest, GenerationContext

# Create RAG generator for HR tenant
generator = GeneratorFactory.create_generator(
    tenant_type=TenantType.HR,
    config={
        "llm_config": {
            "provider_type": "openai",
            "api_key": "your-api-key",
            "model": "gpt-4o-mini"
        },
        "vector_store_config": {
            "store_type": "qdrant",
            "host": "localhost",
            "port": 6333,
            "collection_name": "hr_documents"
        }
    }
)

# Initialize generator
await generator.initialize()

# Create generation request
request = GenerationRequest(
    query="What is our vacation policy?",
    context_documents=["doc_1", "doc_2", "doc_3"],
    parameters={"temperature": 0.3, "max_tokens": 500}
)

context = GenerationContext(
    project_id="project_123",
    user_id="user_456",
    tenant_type="hr",
    task_type="rag"
)

# Generate response
response = await generator.generate(request, context)
print(f"Generated: {response.content}")
print(f"Sources: {response.citations}")
```

### **2. Evaluating Generated Content**

```python
# Example: Evaluating generated content
from factories.evaluator_factory import EvaluatorFactory
from abstractions.evaluator import EvaluationRequest

# Create LLM judge evaluator
evaluator = EvaluatorFactory.create_evaluator(
    evaluator_type="llm_judge",
    config={
        "llm_config": {
            "provider_type": "openai",
            "api_key": "your-api-key",
            "model": "gpt-4o"
        }
    }
)

# Initialize evaluator
await evaluator.initialize()

# Create evaluation request
eval_request = EvaluationRequest(
    generated_content=response.content,
    query=request.query,
    context_documents=["document content 1", "document content 2"],
    criteria={
        "relevance": 0.3,
        "accuracy": 0.3,
        "completeness": 0.2,
        "clarity": 0.2
    }
)

# Evaluate content
eval_result = await evaluator.evaluate(eval_request)
print(f"Overall Score: {eval_result.overall_score}")
print(f"Strengths: {eval_result.strengths}")
print(f"Suggestions: {eval_result.suggestions}")
```

---

## 🧪 **Testing Strategy**

### **1. Abstract Test Cases**

```python
# tests/test_abstractions/test_llm_provider.py
import pytest
from abc import ABC
from abstractions.llm import LLMProvider, LLMRequest, LLMMessage


class LLMProviderTestSuite(ABC):
    """Abstract test suite for LLM providers."""
    
    @pytest.fixture
    @abstractmethod
    def provider(self) -> LLMProvider:
        """Provider fixture to be implemented by concrete tests."""
        pass
    
    @pytest.mark.asyncio
    async def test_initialization(self, provider):
        """Test provider initialization."""
        await provider.initialize()
        assert await provider.health_check()
    
    @pytest.mark.asyncio
    async def test_generation(self, provider):
        """Test content generation."""
        await provider.initialize()
        
        request = LLMRequest(
            messages=[
                LLMMessage(role="user", content="Hello, how are you?")
            ],
            model="gpt-4o-mini",
            temperature=0.7,
            max_tokens=100
        )
        
        response = await provider.generate(request)
        
        assert response.content
        assert response.model
        assert response.usage
        assert response.finish_reason
    
    @pytest.mark.asyncio
    async def test_embedding(self, provider):
        """Test text embedding."""
        await provider.initialize()
        
        embedding = await provider.get_embedding("Hello world")
        
        assert isinstance(embedding, list)
        assert len(embedding) > 0
        assert all(isinstance(x, float) for x in embedding)
    
    def test_supported_models(self, provider):
        """Test supported models list."""
        models = provider.get_supported_models()
        assert isinstance(models, list)
        assert len(models) > 0
```

### **2. Concrete Implementation Tests**

```python
# tests/test_implementations/test_openai_provider.py
import pytest
from implementations.llm.openai_provider import OpenAIProvider
from tests.test_abstractions.test_llm_provider import LLMProviderTestSuite


class TestOpenAIProvider(LLMProviderTestSuite):
    """Test suite for OpenAI provider."""
    
    @pytest.fixture
    def provider(self):
        """Create OpenAI provider for testing."""
        config = {
            "api_key": "test-api-key",
            "timeout": 30
        }
        return OpenAIProvider(config)
    
    @pytest.mark.asyncio
    async def test_openai_specific_features(self, provider):
        """Test OpenAI-specific features."""
        await provider.initialize()
        
        # Test tool calling
        request = LLMRequest(
            messages=[
                LLMMessage(role="user", content="What's the weather like?")
            ],
            model="gpt-4o-mini",
            tools=[
                {
                    "type": "function",
                    "function": {
                        "name": "get_weather",
                        "description": "Get weather information",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "location": {"type": "string"}
                            }
                        }
                    }
                }
            ]
        )
        
        response = await provider.generate(request)
        assert response.tool_calls is not None
```

---

## 📈 **Implementation Progress**

### **✅ Completed Design Elements**
- [x] 🧬 Abstract base classes for all components
- [x] 🏭 Factory patterns for component creation
- [x] 🔧 Concrete implementation examples
- [x] 🧪 Abstract test case patterns
- [x] 📚 Usage examples and documentation

### **⏳ Next Steps**
1. **Implementation Phase:** Build all concrete implementations
2. **Testing Phase:** Create comprehensive test suite
3. **Integration Phase:** Integrate with main API
4. **Documentation Phase:** Complete API documentation
5. **Optimization Phase:** Performance tuning and monitoring

---

*This core library design provides a robust, extensible foundation for TinyRAG v1.4 with clean abstractions, pluggable implementations, and comprehensive testing strategies.* 